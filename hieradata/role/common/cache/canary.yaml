bbr_congestion_control: true
varnish::dynamic_backend_caches: false
cache::lua_support: true
cluster: cache_canary
cache::cluster: text
admin::groups:
  - perf-roots
  - varnish-log-readers
prometheus::node_exporter::collectors_extra:
  - qdisc
  - meminfo_numa
mtail::ensure: 'stopped'
cache::websocket_support: true
# The contents of this hash control our DC->DC routing for varnish backend
# daemons.  There should be a key for every cache datacenter.  The values must
# be a core datacenter (eqiad or codfw), or at least must lead indirectly to
# a core datacenter when traversing the table recursively.  A loop between
# the two core datacenters is expected and normal here.  The only reason to
# edit this is to remove a datacenter from active service (due to fault or
# maintenance) by routing around it from the edge sites.
#
cache::route_table:
  eqiad: 'codfw'
  codfw: 'eqiad'
  ulsfo: 'codfw'
  esams: 'eqiad'
  eqsin: 'codfw'
cache::app_def_be_opts:
  port: 80
  connect_timeout: '5s'
  first_byte_timeout: '180s'
  between_bytes_timeout: '60s'
  max_connections: 0
cache::app_directors:
  appservers:
    backends:
      eqiad: 'appservers.svc.eqiad.wmnet'
      #codfw: 'appservers.svc.codfw.wmnet'
  api:
    backends:
      eqiad: 'api.svc.eqiad.wmnet'
      #codfw: 'api.svc.codfw.wmnet'
  security_audit:
    backends:
      eqiad: 'appservers.svc.eqiad.wmnet'
  appservers_debug:
    be_opts:
      max_connections: 20
    backends:
      eqiad: 'hassium.eqiad.wmnet'
  restbase_backend:
    be_opts:
      port: 7231
      max_connections: 5000
    backends:
      eqiad: 'restbase.svc.eqiad.wmnet'
  cxserver_backend:
    be_opts:
      port: 8080
    backends:
      eqiad: 'cxserver.svc.eqiad.wmnet'
  krypton:
    backends:
      eqiad: 'krypton.eqiad.wmnet'
  phabricator:
    backends:
      eqiad: 'phab1003.eqiad.wmnet'
      # codfw: 'phab2001.codfw.wmnet'
  phab_aphlict:
    be_opts:
      port: 22280
      max_connections: 1000
    backends:
      eqiad: 'phab1003-aphlict.eqiad.wmnet'
      # codfw: 'phab2001-aphlict.codfw.wmnet'
  pybal_config:
    backends:
      eqiad: 'puppetmaster1001.eqiad.wmnet'
      codfw: 'puppetmaster2001.codfw.wmnet'
cache::req_handling:
  cxserver.wikimedia.org:
    director: 'cxserver_backend'
    caching: 'pass'
  default:
    director: 'appservers'
    debug_director: 'appservers_debug'
    subpaths:
      '^/api/rest_v1/':
        director: 'restbase_backend'
      '^/w/api\.php':
        director: 'api'
        debug_director: 'appservers_debug'
cache::alternate_domains:
  config-master.wikimedia.org:
    director: 'pybal_config'
    caching: 'pass'
  grafana.wikimedia.org:
    director: 'krypton'
    caching: 'pass'
  phabricator.wikimedia.org:
    director: 'phabricator'
    subpaths:
      '^/ws/':
        director: 'phab_aphlict'
        caching: 'websockets'
  phab.wmfusercontent.org:
    director: 'phabricator'
# Profile::cache::base
# NOTE: Add the public WMCS IP space when T209011 is done
profile::cache::base::extra_nets: ['172.16.0.0/12']
profile::cache::base::zero_site: 'https://zero.wikimedia.org'
profile::cache::varnish::backend::storage_parts: ['sda3', 'sdb3']
profile::cache::base::fe_runtime_params:
    - default_ttl=86400
profile::cache::base::be_runtime_params:
    - default_ttl=86400
    - nuke_limit=1000
    - lru_interval=31
profile::cache::base::admission_policy: 'none'
profile::cache::base::allow_iptables: true
profile::cache::varnish::cache_be_opts:
    port: 3128
    connect_timeout: '3s'
    first_byte_timeout: '65s'
    between_bytes_timeout: '33s'
    max_connections: 50000
    probe: 'varnish'
profile::cache::varnish::common_vcl_config:
    allowed_methods: '^(GET|HEAD|OPTIONS|POST|PURGE|PUT|DELETE)$'
    # accept "not upload" purge traffic
    purge_host_regex: '^(?!(upload|maps)\.wikimedia\.org)'
    static_host: 'en.wikipedia.org'
    top_domain: 'org'
    shortener_domain: 'w.wiki'
    pass_random: true
profile::cache::varnish::separate_vcl: ['misc']
profile::cache::varnish::frontend::fe_vcl_config:
    enable_geoiplookup: true
    admission_policy: 'none'
    # RTT is ~0, but 100ms is to accomodate small local hiccups, similar to
    # the +100 added in $::profile::cache::base::core_probe_timeout_ms
    varnish_probe_ms: 100
    keep: '1d'
profile::cache::varnish::frontend::fe_jemalloc_conf: 'lg_dirty_mult:8,lg_chunk:16'
profile::cache::varnish::frontend::fe_extra_vcl: ['text-common', 'misc-common', 'normalize_path', 'geoip']
profile::cache::varnish::frontend::transient_gb: 5
profile::cache::varnish::backend::be_vcl_config:
    pass_random: true
profile::cache::varnish::backend::be_extra_vcl: ['text-common', 'misc-common', 'normalize_path']
# Profile::cache::ssl::unified
profile::cache::ssl::unified::monitoring: true
profile::cache::ssl::unified::letsencrypt: false
profile::cache::ssl::unified::acme_chief: true
# Profile::cache::ssl::wikibase
profile::cache::ssl::wikibase::monitoring: true

# Enable varnishkafka-* instance monitoring.
profile::cache::kafka::webrequest::monitoring_enabled: true
profile::cache::kafka::eventlogging::monitoring_enabled: true
profile::cache::kafka::statsv::monitoring_enabled: true

profile::cache::kafka::webrequest::kafka_cluster_name: jumbo-eqiad
profile::cache::kafka::webrequest::ssl_enabled: true

# This should match an entry in the kafka_clusters hash (defined in common.yaml).
# We use the fully qualified kafka cluster name (with datacenter), because we want
# to route all statsv -> statsd traffic to the datacenter that hosts the master
# statsd instance.  If the active statsd instance changes to codfw (for an extended period of time)
# should probably change this to main-codfw.  If you don't things will probably be fine,
# but statsv will have to send messages over UDP cross-DC to the active statsd instance.
profile::cache::kafka::statsv::kafka_cluster_name: main-eqiad

# Monitor varnishkafka-eventlogging process.
profile::cache::kafka::eventlogging::monitoring_enabled: true
profile::cache::kafka::eventlogging::kafka_cluster_name: jumbo-eqiad
profile::cache::kafka::eventlogging::ssl_enabled: true
diamond::remove: true
