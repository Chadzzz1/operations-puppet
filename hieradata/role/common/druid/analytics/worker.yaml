profile::standard::admin_groups:
  - druid-admins

cluster: 'druid_analytics'

prometheus::node_exporter::collectors_extra:
  - meminfo_numa

# Avoid an explicit pin to the cloudera apt component that can
# force packages shared between the cdh and debian upstream distribution
# to prefer the cdh version (we do it only for Hadoop nodes).
profile::cdh::apt::pin_release: false

profile::hadoop::common::hadoop_cluster_name: 'analytics-hadoop'
# Druid nodes get their own Zookeeper cluster to isolate them
# from the production ones.
# Configure the zookeeper profile.
profile::zookeeper::cluster_name: druid-analytics-eqiad
# To avoid version conflics with Cloudera zookeeper package, this
# class manually specifies which debian package version should be installed.
profile::zookeeper::zookeeper_version: '3.4.13-2'
profile::zookeeper::override_java_home: '/usr/lib/jvm/java-11-openjdk-amd64'
# Tranquillity runs via Spark on the Hadoop worker nodes and needs to
# communicate with zookeeper to discover the Druid Overlord master.
# This should be solved in https://github.com/druid-io/tranquility/issues/251
# for Druid >= 0.11
profile::zookeeper::firewall::srange: '$ANALYTICS_NETWORKS'
profile::zookeeper::prometheus_instance: 'analytics'
profile::zookeeper::monitoring_enabled: true

profile::kerberos::keytabs::keytabs_metadata:
  - role: 'druid'
    owner: 'druid'
    group: 'druid'
    filename: 'druid.keytab'


profile::prometheus::druid_exporter::druid_version: '0.19.0'

# -- Druid common configuration

# The logical name of this druid cluster
profile::druid::common::druid_cluster_name: analytics-eqiad
# The logical name of the zookeeper cluster that druid should use
profile::druid::common::zookeeper_cluster_name: druid-analytics-eqiad

# The default MySQL Druid metadata storage database name is just 'druid'.
# Since the analytics-eqiad Druid cluster was originally the only one,
# We set this to the default of 'druid', just to be explicit about it.
profile::druid::common::metadata_storage_database_name: 'druid'

profile::druid::daemons_autoreload: false
profile::druid::ferm_srange: '$ANALYTICS_NETWORKS'

profile::druid::common::properties:
  druid.hadoop.security.kerberos.principal: "druid/%{::fqdn}@WIKIMEDIA"
  druid.hadoop.security.kerberos.keytab: '/etc/security/keytabs/druid/druid.keytab'
  druid.metadata.storage.type: mysql
  druid.metadata.storage.connector.host: an-coord1001.eqiad.wmnet
  druid.metadata.mysql.ssl.useSSL: true
  druid.metadata.mysql.ssl.enabledTLSProtocols: ["TLSv1.2"]
  # druid.metadata.storage.connector.password is set in the private repo.
  druid.storage.type: hdfs
  druid.request.logging.type: slf4j
  druid.request.logging.dir: /var/log/druid
  # Historically, the analytics-eqiad Druid cluster was the only one,
  # and as such had a deep storage directory in HDFS without
  # the cluster name in the path.
  # NOTE: This directory is ensured to exist by usage of the
  # druid::cdh::hadoop::deep_storage define included in the
  # role::analytics_cluster::hadoop::master class.
  druid.storage.storageDirectory: /user/druid/deep-storage
  druid.extensions.loadList:
    - 'druid-datasketches'
    - 'druid-hdfs-storage'
    - 'druid-histogram'
    - 'druid-lookups-cached-global'
    - 'mysql-metadata-storage'
    - 'druid-parquet-extensions'
    - 'druid-avro-extensions'
    - 'druid-kafka-indexing-service'

# -- Druid worker service configurations

# --- Druid Broker
# Broker gets a special ferm_srange since it is the frontend query interface to Druid.
profile::druid::broker::monitoring_enabled: true
profile::druid::broker::ferm_srange: '$ANALYTICS_NETWORKS'
profile::druid::broker::properties:
  druid.emitter: composing
  druid.emitter.composing.emitters: ["logging","http"]
  druid.emitter.http.recipientBaseUrl: "http://127.0.0.1:8000/"
  druid.processing.numThreads: 10
  druid.processing.buffer.sizeBytes: 268435456 # 1024 * 1024 * 256
  # Set numMergeBuffers to use v2 groupBy engine
  druid.processing.numMergeBuffers: 10
  druid.server.http.numThreads: 20
  druid.broker.http.numConnections: 20
  # To avoid excessive queueing on the Historicals,
  # the Broker fails fast if it doesn't receive any data
  # from the Historical after 10s (as opposed to minutes
  # like set in the defaults) or if a query takes more than
  # 10s to complete.
  druid.broker.http.readTimeout: PT10S
  druid.server.http.defaultQueryTimeout: 10000
  # Increase druid broker query cache size to 2G.
  # TBD: Perhaps we should also try using memcached?
  druid.cache.sizeInBytes: 2147483648
  druid.sql.enable: true
  druid.monitoring.monitors: ["org.apache.druid.client.cache.CacheMonitor", "org.apache.druid.server.metrics.QueryCountStatsMonitor"]
profile::druid::broker::env:
  DRUID_HEAP_OPTS: "-Xmx10g -Xms5g"
  DRUID_EXTRA_JVM_OPTS: "-XX:NewSize=4g -XX:MaxNewSize=4g -XX:MaxDirectMemorySize=12g -XX:+UseConcMarkSweepGC -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -Djava.io.tmpdir=/srv/druid/tmp"

# --- Druid Coordinator
profile::druid::coordinator::monitoring_enabled: true
profile::druid::coordinator::ferm_srange: '$ANALYTICS_NETWORKS'
profile::druid::coordinator::properties:
  druid.emitter: composing
  druid.emitter.composing.emitters: ["logging","http"]
  druid.emitter.http.recipientBaseUrl: "http://127.0.0.1:8000/"
profile::druid::coordinator::env:
  DRUID_HEAP_OPTS: "-Xmx2g -Xms2g"
  DRUID_EXTRA_JVM_OPTS: "-XX:NewSize=512m -XX:MaxNewSize=512m -XX:+UseG1GC -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -Djava.io.tmpdir=/srv/druid/tmp"


# --- Druid Historical
profile::druid::historical::monitoring_enabled: true
profile::druid::historical::properties:
  druid.emitter: composing
  druid.emitter.composing.emitters: ["logging","http"]
  druid.emitter.http.recipientBaseUrl: "http://127.0.0.1:8000/"
  druid.processing.buffer.sizeBytes: 268435456 # 1024 * 1024 * 256
  # Set numMergeBuffers to use v2 groupBy engine
  druid.processing.numMergeBuffers: 10
  # 20 Druid http client threads * 5 brokers + 10
  # The value 20 is the lowest among all the node hw types.
  # We need to pick the lowest since each historical needs to be able
  # to sustain, at worst, the maximum amount of concurrent reqs from all
  # brokers plus a safe margin.
  druid.server.http.numThreads: 110
  druid.server.maxSize: 2748779069440 # 2.5 TB
  druid.segmentCache.locations: '[{"path":"/srv/druid/segment-cache","maxSize"\:2748779069440}]'
  # For small clusters it is reccomended to only enable caching on brokers
  # See: http://druid.io/docs/latest/querying/caching.html
  druid.historical.cache.useCache: false
  druid.historical.cache.populateCache: false
  # Sane value to abort a query that takes more than 10s to complete
  druid.server.http.defaultQueryTimeout: 10000
  druid.monitoring.monitors: ["org.apache.druid.server.metrics.HistoricalMetricsMonitor", "org.apache.druid.server.metrics.QueryCountStatsMonitor", "org.apache.druid.client.cache.CacheMonitor"]
profile::druid::historical::env:
  DRUID_HEAP_OPTS: "-Xmx8g -Xms4g"
  # Note -XX:MaxDirectMemorySize is calculated in the profile together with druid.processing.numThreads
  # basing the calculation on the number of available cores on the host.
  DRUID_EXTRA_JVM_OPTS: "-XX:NewSize=2g -XX:MaxNewSize=2g -XX:+UseG1GC -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -Djava.io.tmpdir=/srv/druid/tmp"


# --- Druid MiddleManager
profile::druid::middlemanager::monitoring_enabled: true
profile::druid::middlemanager::properties:
  druid.emitter: composing
  druid.emitter.composing.emitters: ["logging","http"]
  druid.emitter.http.recipientBaseUrl: "http://127.0.0.1:8000/"
  druid.worker.ip: "%{::fqdn}"
  druid.worker.capacity: 12
  druid.processing.numThreads: 3
  druid.processing.buffer.sizeBytes: 356515840
  druid.server.http.numThreads: 20
  druid.indexer.runner.javaOpts: "-server -Xmx2g -XX:+UseG1GC -XX:MaxGCPauseMillis=100 -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -Dhadoop.mapreduce.job.classloader=true"
profile::druid::middlemanager::env:
  DRUID_HEAP_OPTS: "-Xmx64m -Xms64m"
  DRUID_EXTRA_JVM_OPTS: "-XX:+UseConcMarkSweepGC -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -Djava.io.tmpdir=/srv/druid/tmp"


# --- Druid Overlord
# Overlord will accept indexing jobs from Hadoop nodes in the ANALYTICS_NETWORKS
profile::druid::overlord::monitoring_enabled: true
profile::druid::overlord::ferm_srange: '$ANALYTICS_NETWORKS'
profile::druid::overlord::properties:
  druid.indexer.runner.type: remote
  druid.indexer.storage.type: metadata
profile::druid::overlord::env:
  DRUID_HEAP_OPTS: "-Xmx1g -Xms1g"
  DRUID_EXTRA_JVM_OPTS: "-XX:NewSize=256m -XX:MaxNewSize=256m -XX:+UseConcMarkSweepGC -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -Djava.io.tmpdir=/srv/druid/tmp"

profile::java::java_packages:
  - version: '8'
    variant: 'jdk'
  - version: '11'
    variant: 'jdk'
profile::java::extra_args: 'JAVA_TOOL_OPTIONS="-Dfile.encoding=UTF-8"'
