# General configs
cluster: elasticsearch
admin::groups:
  - elasticsearch-roots

# T130329
profile::base::check_disk_options: -w 15% -c 10% -W 6% -K 3% -l -e -A -i "/srv/sd[a-b][1-3]" --exclude-type=tracefs

profile::elasticsearch::cirrus::storage_device: md1
profile::elasticsearch::cirrus::ferm_srange: '$DOMAIN_NETWORKS'
profile::elasticsearch::instances: {}
profile::elasticsearch::base_data_dir: /srv/elasticsearch
profile::elasticsearch::common_settings:
    awareness_attributes: 'row'
    # We need these plugins to be loaded in order to work properly. This will keep
    # elasticsearch from starting if these plugins are not available.
    plugins_mandatory:
      - experimental-highlighter
      - extra
      - analysis-icu

    # More than 30G isn't very useful
    heap_memory: '30G'

    # Don't run if more than 1 master missing
    minimum_master_nodes: 2

    # wait that long for all nodes to restart. If not all nodes are present after
    # `recover_after_time`, recover anyway, as long as at least
    # `recover_after_nodes` are present.
    recover_after_time: '5m'

    # mwgrep queries one copy of each shard in the cluster, which is currently
    # just over 3k shards. For it to work we need to increase the limit from
    # default 1k
    search_shard_count_limit: 5000

    # By default elasticsearch sets this to ((# of available_processors * 3) /
    # 2) + 1, which works out to 49 for our servers. When search is stalled on
    # IO it makes sense to have more threads than cores, but we never see our
    # servers stalled out on IO.  We have seen (T169498) servers completely
    # exhaust their CPU though, so setting this to match cpu counts to prevent
    # overcommit of CPU resources.
    search_thread_pool_executors: 32

    # Increase the per-node cache for compiled LTR models from default 10MB
    ltr_cache_size: '100mb'

    http_port: 9200
    tls_port: 9243
    transport_tcp_port: 9300

    # Let apifeatureusage create their indices
    auto_create_index: '+apifeatureusage-*,-*'

    script_max_compilations_per_minute: 10000
    bulk_thread_pool_executors: 6
    bulk_thread_pool_capacity: 1000
    filter_cache_size: '20%'

# Msearch daemons read same topic in same consumer group in all dcs. They
# toggle themselves on/off based on load of the local elasticsearch cluster.
profile::mjolnir::kafka_msearch_daemon::kafka_cluster: jumbo-eqiad
profile::mjolnir::kafka_msearch_daemon::input_topic: mjolnir.msearch-prod-request
profile::mjolnir::kafka_msearch_daemon::output_topic: mjolnir.msearch-prod-response
# Max concurrent search threads consumed is:
#   # kafka partitions * num_workers * max_concurrent_searches * # shards per index
#   = (35 * 2 * 2 * 7)
#   = max 980 concurrent shard searches
# Unfortunately # shards per index is variable, so some caution is still required.
profile::mjolnir::kafka_msearch_daemon::num_workers: 2
profile::mjolnir::kafka_msearch_daemon::max_concurrent_searches: 2

# Bulk daemon consumes per-datacenter to apply page updates to all clusters.
profile::mjolnir::kafka_bulk_daemon::kafka_cluster: "main-%{::site}"
profile::mjolnir::kafka_bulk_daemon::group_id: "cirrussearch_updates_%{::site}"
profile::mjolnir::kafka_bulk_daemon::topics:
  - eqiad.cirrussearch.page-index-update
  - codfw.cirrussearch.page-index-update
