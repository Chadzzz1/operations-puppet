nagios_group: analytics_eqiad
cluster: analytics
# FIXME:
# indirect hiera lookup due to includes in the role:
# role::analytics::hadoop::client
hadoop_zookeeper_cluster_name: main-eqiad
admin::groups:
  - analytics-admins
admin::groups_no_ssh:
  - analytics-users
  - analytics-privatedata-users
  # elasticsearch::analytics creates the analytics-search user and group
  # that analytics-search-users are allowed to sudo to.  This is used
  # for deploying files to HDFS.
  - analytics-search-users

profile::hadoop::common::hadoop_cluster_name: 'analytics-test-hadoop'

profile::hadoop::standby_master::monitoring_enabled: true
profile::hadoop::logstash::enabled: false

profile::hadoop::common::config_override:
  # This is the heap zize of YARN daemons ResourceManager and NodeManagers.
  # This setting is used to configure the max heap size for both.
  # The default is 1000m, we increase it to 2048m in production.
  yarn_heapsize: 2048
  # Prometheus JMX Exporter config templates.
  hadoop_namenode_opts: "-Xms12288m -Xmx12288m -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled -javaagent:/usr/share/java/prometheus/jmx_prometheus_javaagent.jar=%{::ipaddress}:10080:/etc/prometheus/hdfs_namenode_jmx_exporter.yaml"
  yarn_resourcemanager_opts: "-Xms4096m -Xmx4096m -javaagent:/usr/share/java/prometheus/jmx_prometheus_javaagent.jar=%{::ipaddress}:10083:/etc/prometheus/yarn_resourcemanager_jmx_exporter.yaml"
  # The HDFS Trash is configured in this way:
  # 1) Once every day a checkpoint is made (that contains all the trash for a day).
  # 2) After a month a checkpoint is deleted.
  hdfs_trash_checkpoint_interval: 1440
  hdfs_trash_interval: 43200
  hdfs_site_extra_properties:
    dfs.http.policy: 'HTTPS_ONLY'
  mapred_site_extra_properties:
    mapreduce.ssl.enabled: true
    mapreduce.shuffle.ssl.enabled: true
    mapreduce.jobhistory.http.policy: 'HTTPS_ONLY'
  yarn_site_extra_properties:
    yarn.http.policy: 'HTTPS_ONLY'
  core_site_extra_properties:
    hadoop.ssl.enabled: true
    hadoop.ssl.require.client.cert: false
    hadoop.ssl.hostname.verifier: false
    hadoop.ssl.keystores.factory.class: 'org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory'
    hadoop.ssl.server.conf: 'ssl-server.xml'
    hadoop.ssl.client.conf: 'ssl-client.xml'
    hadoop.ssl.enabled.protocols: 'TLSv1,TLSv1.1,TLSv1.2'

profile::analytics::database::meta::backup_dest::hive::metastore_host: 'analytics1030.eqiad.wmnet'
profile::analytics::database::meta::backup_dest::oozie_host: 'analytics1030.eqiad.wmnet'

profile::hadoop::firewall::master::analytics_srange: '$ANALYTICS_NETWORKS'
profile::hadoop::firewall::master::analytics_druid_srange: '(($ANALYTICS_NETWORKS $DRUID_PUBLIC_HOSTS))'
profile::hadoop::firewall::master::hdfs::ssl_enabled: true
profile::hadoop::firewall::master::yarn::ssl_enabled: true
profile::hadoop::firewall::master::mapred::ssl_enabled: true

profile::hadoop::backup::namenode::monitoring_enabled: true

diamond::remove: true

profile::base::notifications: disabled