#
# There's only one Hadoop cluster in cloud-analytics, so we
# are safe to set this stuff globally for the cloud-analytics Cloud VPS project.
#
# NOTE: I would put these in Horizon Hiera interface if it were not for
#       https://phabricator.wikimedia.org/T177855
#

# Hadoop clients will connect to the Hadoop cluster by this name.
profile::hadoop::common::hadoop_cluster_name: cloud-analytics

# Ensure that users in these posix groups have home directories in HDFS.
profile::hadoop::master::hadoop_user_groups: project-cloud-analytics


# Zookeeper clients will connect to the Zookeeper cluster by this name.
profile::zookeeper::cluster_name: cloud-analytics
# To avoid version conflics with Cloudera zookeeper package, this
# class manually specifies which debian package version should be installed.
profile::zookeeper::zookeeper_version: '3.4.9-3+deb9u1'

# List of zookeeper clusters.
zookeeper_clusters:
  cloud-analytics:
    hosts:
      ca-conf-1.cloud-analytics.eqiad.wmflabs: '1'
      ca-conf-2.cloud-analytics.eqiad.wmflabs: '2'


# Hadoop base configuration is common to multiple profiles, and must be kept
# in sync. Instead of having it repated multiple times it is convenient to
# have a single place in hiera to check/modify.
hadoop_clusters:
  cloud-analytics:
    zookeeper_cluster_name: cloud-analytics
    resourcemanager_hosts:
      - ca-master-1.cloud-analytics.eqiad.wmflabs
      - ca-master-2.cloud-analytics.eqiad.wmflabs
    namenode_hosts:
      - ca-master-1.cloud-analytics.eqiad.wmflabs
      - ca-master-2.cloud-analytics.eqiad.wmflabs
    journalnode_hosts:
      - ca-worker-1.cloud-analytics.eqiad.wmflabs
      - ca-worker-2.cloud-analytics.eqiad.wmflabs
      - ca-worker-3.cloud-analytics.eqiad.wmflabs
      - ca-worker-4.cloud-analytics.eqiad.wmflabs
      - ca-worker-5.cloud-analytics.eqiad.wmflabs
    hadoop_var_directory: /srv/hadoop
    datanode_mounts:
      - /srv/hadoop/data/a

    # Configure memory based on these recommendations and then adjusted:
    # http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.0.6.0/bk_installing_manually_book/content/rpm-chap1-11.html

    # These Map/Reduce and YARN ApplicationMaster master settings are
    # also settable per job.
    # All worker nodes have 120G Ram and 46 cores.
    # We will primarily be running Presto on these nodes,
    # Not jobs in YARN, so we can adjust to make Hadoop use minimal resources.

    # Choosing 512G for default application container size.
    # Map container size and JVM max heap size (-XmX)
    mapreduce_map_memory_mb: 512
    mapreduce_map_java_opts: '-Xmx410m'  # 0.8 * 512M

    # Reduce container size and JVM max heap size (-Xmx)
    mapreduce_reduce_memory_mb: 1024         # 2 * 512M
    mapreduce_reduce_java_opts: '-Xmx820m'  # 0.8 * 2 * 512M

    # Yarn ApplicationMaster container size and  max heap size (-Xmx)
    yarn_app_mapreduce_am_resource_mb: 1024          # 2 * 512G
    yarn_app_mapreduce_am_command_opts: '-Xmx820m'  # 0.8 * 2 * 512M

    # Save a lot of non Hadoop memory for Presto and other processes.
    yarn_nodemanager_os_reserved_memory_mb: 106496 # 104G

    # We use 8G total RAM and 4 cores for YARN

    # Allow a job to request up to the smallest value of yarn_nodemanager_resource_memory_mb (120G - 104G = 16G)
    # in the cluster. The smallest value is 52G on the R720s (analytics1069 and below).
    yarn_scheduler_maximum_allocation_mb: 16384
    yarn_scheduler_minimum_allocation_vcores:   0
    yarn_scheduler_maximum_allocation_vcores:   4
    # Raised for T206943
    yarn_resourcemanager_zk_timeout_ms: 20000

    # Since we are not sharing Zookeper with another cluster
    # the Yarn rmstore path can stay unchanged.
    yarn_resourcemanager_zk_state_store_parent_path: '/rmstore'

    # Hadoop process heapsizes and Prometheus JMX Exporter config templates.
    hadoop_namenode_opts: "-Xms1024m -Xmx1024m -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled -javaagent:/usr/share/java/prometheus/jmx_prometheus_javaagent.jar=%{::ipaddress}:10080:/etc/prometheus/hdfs_namenode_jmx_exporter.yaml"
    yarn_resourcemanager_opts: "-Xms512m -Xmx512m -javaagent:/usr/share/java/prometheus/jmx_prometheus_javaagent.jar=%{::ipaddress}:10083:/etc/prometheus/yarn_resourcemanager_jmx_exporter.yaml"
    mapreduce_history_java_opts: "-Xms128m -Xmx128m -javaagent:/usr/share/java/prometheus/jmx_prometheus_javaagent.jar=%{::ipaddress}:10086:/etc/prometheus/mapreduce_history_jmx_exporter.yaml"
    hadoop_datanode_opts: "-Xms4096m -Xmx4096m -javaagent:/usr/share/java/prometheus/jmx_prometheus_javaagent.jar=%{::ipaddress}:51010:/etc/prometheus/hdfs_datanode_jmx_exporter.yaml"
    hadoop_journalnode_opts: "-javaagent:/usr/share/java/prometheus/jmx_prometheus_javaagent.jar=%{::ipaddress}:10485:/etc/prometheus/hdfs_journalnode_jmx_exporter.yaml"
    yarn_nodemanager_opts: "-Xms2048m -Xmx2048m -javaagent:/usr/share/java/prometheus/jmx_prometheus_javaagent.jar=[::]:8141:/etc/prometheus/yarn_nodemanager_jmx_exporter.yaml"

    # The HDFS Trash is configured in this way:
    # 1) Once every day a checkpoint is made (that contains all the trash for a day).
    # 2) After a week a checkpoint is deleted.
    hdfs_trash_checkpoint_interval: 1440
    hdfs_trash_interval: 10080

    # Allow presto-server to impersonate users.
    # This is what allows presto to access files as users running queries.
    core_site_extra_properties:
      hadoop.proxyuser.presto.hosts: '*'
      hadoop.proxyuser.presto.groups: '*'

# Set monitoring_enabled to true for all Hadoop processes.
profile::hadoop::master::monitoring_enabled: true
profile::hadoop::worker::monitoring_enabled: true
profile::hadoop::standby_master::monitoring_enabled: true


#
# Presto settings:
#

# This will be used for the node.enviroment for all Presto servers in this cluster.
profile::presto::cluster_name: cloud-analytics

# TODO: Set up LVS for discovery since any Presto node can do it?
profile::presto::discovery_uri: http://ca-master-2.cloud-analytics.eqiad.wmflabs:8280

# Use a lot of memory for Presto workers.
# This should match the value of Hadoop yarn_nodemanager_os_reserved_memory_mb
profile::presto::server::heap_max: 104G

profile::presto::server::catalogs:
  # Each catalog hash should contain a single properties has that will
  # end up being passed to the presto::properties define.  This will render
  # a properties file at /etc/presto/catalog/$name.properties.
  hive:
    properties:
      connector.name: hive-hadoop2
      # Add Hadoop config files so Hive connector can work with HA Hadoop NameNodes.
      hive.config.resources: /etc/hadoop/conf/core-site.xml,/etc/hadoop/conf/hdfs-site.xml
      hive.metastore.uri: thrift://ca-coord-1.cloud-analytics.eqiad.wmflabs:9083
      hive.metastore.username: presto
      hive.storage-format: PARQUET
      hive.compression-codec: SNAPPY
      # We colocate workers with Hadoop DataNodes.
      hive.force-local-scheduling: true
      # Allow presto-cli to impersonate the user running the process
      hive.hdfs.impersonation.enabled: true
      # TODO: do we want to disable non managed tables?
      hive.non-managed-table-writes-enabled: true
      hive.non-managed-table-creates-enabled: true
  # TODO: setup catalog connection to labsdb mysql?



# Hive Client settings.
profile::hive::client::zookeeper_cluster_name: cloud-analytics
profile::hive::client::server_host: ca-coord-1.cloud-analytics.eqiad.wmflabs
profile::hive::client::server_port: 10000
profile::hive::client::metastore_host: ca-coord-1.cloud-analytics.eqiad.wmflabs

# Hive Server Settings
# (These opts are in ::hive::client namespace because profile::hive::client installs the config files.)
profile::hive::client::hive_metastore_opts: '-javaagent:/usr/share/java/prometheus/jmx_prometheus_javaagent.jar=%{::ipaddress}:9183:/etc/prometheus/hive_metastore_jmx_exporter.yaml'
profile::hive::client::hive_server_opts: '-javaagent:/usr/share/java/prometheus/jmx_prometheus_javaagent.jar=%{::ipaddress}:10100:/etc/prometheus/hive_server_jmx_exporter.yaml'
profile::hive::server::monitoring_enabled: true
profile::hive::metastore::monitoring_enabled: true
profile::analytics::database::meta::datadir: /srv/mysql_data
profile::analytics::database::meta::tmpdir: /srv/mysql_tmp
profile::analytics::database::meta::monitoring_enabled: true

