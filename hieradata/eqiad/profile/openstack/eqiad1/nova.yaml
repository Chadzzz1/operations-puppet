profile::openstack::eqiad1::nova::db_host: 'm5-master.eqiad.wmnet'
profile::openstack::eqiad1::nova::db_name: 'nova_eqiad1'
profile::openstack::eqiad1::nova::db_name_api: 'nova_api_eqiad1'
profile::openstack::eqiad1::nova::dhcp_domain: 'eqiad.wmflabs'
profile::openstack::eqiad1::nova::network_flat_tagged_base_interface: 'eth1'
profile::openstack::eqiad1::nova::network_flat_interface_vlan: '1105'
profile::openstack::eqiad1::nova::network_flat_interface: 'eth1.1105'
profile::openstack::eqiad1::nova::instance_network_id: '7425e328-560c-4f00-8e99-706f3fb90bb4'
profile::openstack::eqiad1::nova::physical_interface_mappings:
  cloudinstances2b: 'eth1.1105'


# Nova is permitted to schedule new VMs on the following
#  hosts.  Nodes currently allocated for eqiad1 are
#  cloudvirt1013 through cloudvirt1030.
#
# cloudvirt1025 is depooled so we can drain it and rebuild
#  it with Stretch.
#
# cloudvirt1024 and cloudvirt1030 are reserved as emergency spares.
#  We're saving 1024 rather than the highest- or lowest-numbered
#  server because 1024 has larger disks so will be big
#  enough to accept as refugees all the VMs from any other
#  virt host.
#
# There are some hypervisors that are visible to nova
#  but will never be in the general scheduler pool
#  because they're reserved for hand-created special-purpose
#  instances.  They are:
#
# cloudvirt1019 and 1020: reserved for cloud-wide DB instances
#
# cloudvirtanXXXX: reserved for gigantic cloud-analytics worker nodes
#
#
profile::openstack::eqiad1::nova::scheduler_pool:
  - cloudvirt1013
  - cloudvirt1014
  - cloudvirt1015
  - cloudvirt1016
  - cloudvirt1017
  - cloudvirt1018
  - cloudvirt1021
  - cloudvirt1022
  - cloudvirt1023
  - cloudvirt1026
  - cloudvirt1027
  - cloudvirt1028
  - cloudvirt1029
